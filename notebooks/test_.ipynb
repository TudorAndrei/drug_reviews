{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tudor/cave/drug_reviews/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"../data/raw/drugsComTrain_raw.tsv\", sep=\"\\t\").drop(\"Unnamed: 0\", axis=1)\n",
    "test = pd.read_csv(r\"../data/raw/drugsComTest_raw.tsv\", sep=\"\\t\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review'] = data['review'].str.replace('&#039;',\"'\",)\n",
    "data['review'] = data['review'].str.replace('\"',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['review'] = test['review'].str.replace('&#039;',\"'\",)\n",
    "test['review'] = test['review'].str.replace('\"',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['condition'].str.contains(\"users found this comment helpful\") == False]\n",
    "test = test[test['condition'].str.contains(\"users found this comment helpful\") == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../data/raw/drugs_train.csv\", index=False)\n",
    "test.to_csv(\"../data/raw/drugs_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_[\"rating\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_['review']\n",
    "y = data_['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extractors.\n",
    "feature_extractors = [\n",
    "CountVectorizer(),\n",
    "TfidfVectorizer(),\n",
    "# Ignore words that appear in less than 20% of posts (rare words).\n",
    "CountVectorizer(min_df=0.2),\n",
    "# Ignore words that appear in more than 80% of posts (frequent words).\n",
    "CountVectorizer(max_df=0.8),\n",
    "CountVectorizer(ngram_range=(2, 2)),\n",
    "CountVectorizer(ngram_range=(3, 3)),\n",
    "TfidfVectorizer(min_df=0.2),\n",
    "TfidfVectorizer(max_df=0.8),\n",
    "TfidfVectorizer(ngram_range=(2, 2)),\n",
    "TfidfVectorizer(ngram_range=(3, 3))\n",
    "]\n",
    "\n",
    "# Classifiers.\n",
    "classifiers = [ \n",
    "# LinearSVC(max_iter=100000), \n",
    "# LogisticRegression(max_iter=100000),\n",
    "KNeighborsClassifier(n_jobs=-1),\n",
    "RandomForestClassifier(n_jobs=-1),\n",
    "DecisionTreeClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "scoring = {\n",
    "'f1_score': make_scorer(f1_score, average='weighted')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_ML_pipeline():\n",
    "   for classifier in classifiers:\n",
    "      for extractor in feature_extractors:\n",
    "         start_time = time.time()\n",
    "         pipeline = Pipeline([('extractor', extractor), ('classifier', classifier)])\n",
    "         scores = cross_validate(pipeline, x, y, cv=cv, scoring=scoring)\n",
    "         end_time = time.time()\n",
    "         total_time = round((end_time - start_time)/60, 2)\n",
    "         \n",
    "         f1_score = round(np.mean(scores['test_f1_score']), 2)\n",
    "         # precision = round(np.mean(scores['test_precision']), 2)\n",
    "         # recall = round(np.mean(scores['test_recall']), 2)\n",
    "         # acc = round(np.mean(scores['test_accuracy']), 2)\n",
    "\n",
    "         print(\"Time: \", total_time, \" min\")\n",
    "         print(\"Experiment: \", str(classifier), \"+\", str(extractor))\n",
    "         print(\"F1 score= \", f1_score, \", 5-fold CV=\", scores['test_f1_score'])\n",
    "         # print(\"Precision= \", precision, \", 5-fold CV=\", scores['test_precision'])\n",
    "         # print(\"Recall= \", recall, \", 5-fold CV=\", scores['test_recall'])\n",
    "         # print(\"Accuracy= \", acc, \", 5-fold CV=\", scores['test_accuracy'])\n",
    "         print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ML_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv(r\"../data/drugs_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've tried a few antidepressants over the years (citalopram, fluoxetine, amitriptyline), but none of those helped with my depression, insomnia &amp; anxiety. My doctor suggested and changed me onto 45mg mirtazapine and this medicine has saved my life. Thankfully I have had no side effects especially the most common - weight gain, I've actually lost alot of weight. I still have suicidal thoughts but mirtazapine has saved me.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53200"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import pretty_errors\n",
    "import torch\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch.nn import Dropout, Linear, Module, ReLU, Sequential, Sigmoid\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss, BCELoss,CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.functional import f1\n",
    "from transformers.models.auto.modeling_auto import AutoModelForSequenceClassification\n",
    "from torch.nn.functional import sigmoid, one_hot\n",
    "import logging as log\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = None,\n",
    "        data_frame: str = None,\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.data = data_frame\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data['review'][idx]\n",
    "        label = self.data['rating'][idx]\n",
    "        text = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            # add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            # return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        return {\n",
    "            \"ids\": torch.tensor(text[\"input_ids\"], dtype=torch.long),\n",
    "            \"mask\": torch.tensor(text[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(int(label), dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_workers: int = 8,\n",
    "        batch_size: int = 2,\n",
    "        model_name: str = None,\n",
    "        data_path: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        self.model_name = model_name\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.train, self.val = None, None\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        self.train, self.val = train_test_split(self.data, test_size=0.2, stratify=self.data['rating'])\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            BaseDataset(data_frame=self.train.reset_index(drop=True), model_name=self.model_name),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            BaseDataset(data_frame=self.val.reset_index(drop=True), model_name=self.model_name),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBert(LightningModule):\n",
    "    def __init__(self, model=None, n_classes: int = None) -> None:\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.lr = 0.003\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(model, num_labels=n_classes)\n",
    "        # print(self.bert)\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.freeze_model(self.bert.bert)\n",
    "\n",
    "    def freeze_model(self, model):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.bert(input_ids=ids, attention_mask=mask)\n",
    "        # out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.bert.parameters(), lr=self.lr)\n",
    "        return {\n",
    "            \"optimizer\": optimizer\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        ids, mask, labels = batch[\"ids\"], batch[\"mask\"], batch[\"labels\"]\n",
    "        output = self(ids, mask)\n",
    "        output = output[0]\n",
    "        # output = torch.argmax(output, dim=1)\n",
    "        print(output)\n",
    "        loss = self.criterion(output, labels)\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    # def validation_step(self, batch, _):\n",
    "    #     ids, mask, labels = batch[\"ids\"], batch[\"mask\"], batch[\"labels\"]\n",
    "    #     output = self(ids, mask, labels)\n",
    "    #     print(output)\n",
    "    #     loss = output['loss']\n",
    "    #     logits = output['logits']\n",
    "    #     # loss = self.criterion(output, labels)\n",
    "    #     f1_score = f1(sigmoid(logits), labels.int(), average=\"macro\", num_classes=self.n_classes)\n",
    "    #     return {\"loss\": loss, \"f1\": f1_score}\n",
    "\n",
    "    # def validation_epoch_end(self, out):\n",
    "    #     loss = torch.stack([x[\"loss\"] for x in out]).mean()\n",
    "    #     f1_score = torch.stack([x[\"f1\"] for x in out]).mean()\n",
    "    #     self.log(\"val/val_loss\", loss, on_epoch=True, on_step=False)\n",
    "    #     self.log(\"val/val_f1\", f1_score, on_epoch=True, on_step=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:131: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "Missing logger folder: /home/tudor/cave/drug_reviews/notebooks/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                          | Params\n",
      "------------------------------------------------------------\n",
      "0 | bert      | BertForSequenceClassification | 109 M \n",
      "1 | criterion | CrossEntropyLoss              | 0     \n",
      "------------------------------------------------------------\n",
      "7.7 K     Trainable params\n",
      "109 M     Non-trainable params\n",
      "109 M     Total params\n",
      "437.960   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/63799 [00:00<?, ?it/s]tensor([[-0.0667,  0.2759, -0.4193, -0.9708, -0.4728, -0.1505,  0.0562, -0.3366,\n",
      "          0.0655, -0.0272],\n",
      "        [-0.0670,  0.5937, -0.1746, -0.7611, -0.3530, -0.0722, -0.0641, -0.6967,\n",
      "         -0.0078, -0.3326]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Epoch 0:   0%|          | 1/63799 [00:00<5:24:06,  3.28it/s, loss=2.21, v_num=0]tensor([[-0.7518, -0.2709, -0.8620, -1.1923, -1.0401, -0.7716,  0.4327, -0.8111,\n",
      "         -0.5590,  0.2603],\n",
      "        [-0.6685, -0.2183, -0.8289, -1.3225, -0.9243, -0.5310,  0.5743, -0.8304,\n",
      "         -0.4321,  0.2908]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Epoch 0:   0%|          | 2/63799 [00:00<3:01:50,  5.85it/s, loss=2.54, v_num=0]tensor([[-1.1922, -1.0044, -1.4399, -1.2035, -1.6453, -1.4159,  0.7040, -1.4549,\n",
      "         -0.3291,  0.5455],\n",
      "        [-1.3951, -0.9573, -1.6025, -1.1967, -1.8127, -1.3657,  0.6847, -1.8685,\n",
      "         -0.3991,  0.4052]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1640811806235/work/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1640811806235/work/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/63799 [03:55<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/63799 [02:16<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/63799 [00:34<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=723'>724</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=724'>725</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:812\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=809'>810</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=811'>812</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=813'>814</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1237\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1234'>1235</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1236'>1237</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1238'>1239</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1324\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1322'>1323</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1323'>1324</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1354\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1352'>1353</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1353'>1354</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:269\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=267'>268</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=268'>269</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:208\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=206'>207</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=207'>208</a>\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(batch, batch_idx)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=209'>210</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=86'>87</a>\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, batch_idx)\n\u001b[0;32m---> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=87'>88</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(split_batch, optimizers, batch_idx)\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=88'>89</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=201'>202</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, batch: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=202'>203</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=203'>204</a>\u001b[0m         batch,\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=204'>205</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_idx,\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=205'>206</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position],\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=206'>207</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_idx,\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=207'>208</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=208'>209</a>\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=209'>210</a>\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=210'>211</a>\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:256\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=250'>251</a>\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=251'>252</a>\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=252'>253</a>\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=253'>254</a>\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=254'>255</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=255'>256</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, batch_idx, closure)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=257'>258</a>\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:369\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=367'>368</a>\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=368'>369</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=369'>370</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=370'>371</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=371'>372</a>\u001b[0m     batch_idx,\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=372'>373</a>\u001b[0m     optimizer,\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=373'>374</a>\u001b[0m     opt_idx,\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=374'>375</a>\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=375'>376</a>\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator, TPUAccelerator),\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=376'>377</a>\u001b[0m     using_native_amp\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mamp_backend \u001b[39m==\u001b[39;49m AMPType\u001b[39m.\u001b[39;49mNATIVE),\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=377'>378</a>\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=378'>379</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=380'>381</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1596\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1594'>1595</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1595'>1596</a>\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1597'>1598</a>\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1625\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1553'>1554</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1554'>1555</a>\u001b[0m \u001b[39mOverride this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1555'>1556</a>\u001b[0m \u001b[39meach optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1622'>1623</a>\u001b[0m \n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1623'>1624</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1624'>1625</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:168\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py?line=166'>167</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py?line=167'>168</a>\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py?line=169'>170</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:193\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=191'>192</a>\u001b[0m model \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=192'>193</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(model, optimizer, opt_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:155\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=153'>154</a>\u001b[0m     closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=154'>155</a>\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=27'>28</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/optim/adam.py:92\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/optim/adam.py?line=90'>91</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m---> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/optim/adam.py?line=91'>92</a>\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/optim/adam.py?line=93'>94</a>\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:140\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=133'>134</a>\u001b[0m \u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=134'>135</a>\u001b[0m \u001b[39m``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=135'>136</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=136'>137</a>\u001b[0m \u001b[39mThe closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=137'>138</a>\u001b[0m \u001b[39mconsistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=138'>139</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=139'>140</a>\u001b[0m closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=140'>141</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=146'>147</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=147'>148</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=148'>149</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:143\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=141'>142</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=142'>143</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backward_fn(step_output\u001b[39m.\u001b[39;49mclosure_loss)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=144'>145</a>\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:311\u001b[0m, in \u001b[0;36mOptimizerLoop._make_backward_fn.<locals>.backward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=309'>310</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward_fn\u001b[39m(loss: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=310'>311</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mbackward\u001b[39;49m\u001b[39m\"\u001b[39;49m, loss, optimizer, opt_idx)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=312'>313</a>\u001b[0m     \u001b[39m# check if model weights are nan\u001b[39;00m\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1766\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1764'>1765</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1765'>1766</a>\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1767'>1768</a>\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:168\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[0;34m(self, closure_loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=165'>166</a>\u001b[0m closure_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpre_backward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, closure_loss)\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=167'>168</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module, closure_loss, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=169'>170</a>\u001b[0m closure_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpost_backward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, closure_loss)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:80\u001b[0m, in \u001b[0;36mPrecisionPlugin.backward\u001b[0;34m(self, model, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=78'>79</a>\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[0;32m---> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=79'>80</a>\u001b[0m     model\u001b[39m.\u001b[39;49mbackward(closure_loss, optimizer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=80'>81</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1370\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[0;34m(self, loss, optimizer, optimizer_idx, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1355'>1356</a>\u001b[0m \u001b[39m\"\"\"Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1356'>1357</a>\u001b[0m \u001b[39mown implementation if you need to.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1357'>1358</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1367'>1368</a>\u001b[0m \u001b[39m        loss.backward()\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1368'>1369</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py?line=1369'>1370</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tudor/cave/drug_reviews/notebooks/explore.ipynb Cell 24'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tudor/cave/drug_reviews/notebooks/explore.ipynb#ch0000023?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m BaseBert(model\u001b[39m=\u001b[39mMODEL, n_classes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tudor/cave/drug_reviews/notebooks/explore.ipynb#ch0000023?line=4'>5</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tudor/cave/drug_reviews/notebooks/explore.ipynb#ch0000023?line=5'>6</a>\u001b[0m         gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tudor/cave/drug_reviews/notebooks/explore.ipynb#ch0000023?line=6'>7</a>\u001b[0m         max_epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tudor/cave/drug_reviews/notebooks/explore.ipynb#ch0000023?line=7'>8</a>\u001b[0m     )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tudor/cave/drug_reviews/notebooks/explore.ipynb#ch0000023?line=8'>9</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:771\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=751'>752</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=752'>753</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=753'>754</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=770'>771</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=771'>772</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=772'>773</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:739\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=736'>737</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mreconciliate_processes(traceback\u001b[39m.\u001b[39mformat_exc())\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=737'>738</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_exception\u001b[39m\u001b[39m\"\u001b[39m, exception)\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=738'>739</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_teardown()\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=739'>740</a>\u001b[0m \u001b[39m# teardown might access the stage so we reset it after\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=740'>741</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstage \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1301\u001b[0m, in \u001b[0;36mTrainer._teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1297'>1298</a>\u001b[0m \u001b[39m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1298'>1299</a>\u001b[0m \u001b[39mCallback; those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1299'>1300</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_dispatch(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1300'>1301</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mteardown()\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1301'>1302</a>\u001b[0m loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_active_loop\n\u001b[1;32m   <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1302'>1303</a>\u001b[0m \u001b[39m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/single_device.py:93\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/single_device.py?line=91'>92</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mteardown\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/single_device.py?line=92'>93</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mteardown()\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/single_device.py?line=93'>94</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/single_device.py?line=94'>95</a>\u001b[0m         \u001b[39m# GPU teardown\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/single_device.py?line=95'>96</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:444\u001b[0m, in \u001b[0;36mStrategy.teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=438'>439</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mteardown\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=439'>440</a>\u001b[0m     \u001b[39m\"\"\"This method is called to teardown the training process.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=440'>441</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=441'>442</a>\u001b[0m \u001b[39m    It is the right place to release memory and free other resources.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=442'>443</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=443'>444</a>\u001b[0m     optimizers_to_device(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizers, torch\u001b[39m.\u001b[39;49mdevice(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=444'>445</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mteardown()\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/optimizer.py:27\u001b[0m, in \u001b[0;36moptimizers_to_device\u001b[0;34m(optimizers, device)\u001b[0m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/optimizer.py?line=24'>25</a>\u001b[0m \u001b[39m\"\"\"Moves optimizer states for a sequence of optimizers to the device.\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/optimizer.py?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m opt \u001b[39min\u001b[39;00m optimizers:\n\u001b[0;32m---> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/optimizer.py?line=26'>27</a>\u001b[0m     optimizer_to_device(opt, device)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/optimizer.py:33\u001b[0m, in \u001b[0;36moptimizer_to_device\u001b[0;34m(optimizer, device)\u001b[0m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/optimizer.py?line=30'>31</a>\u001b[0m \u001b[39m\"\"\"Moves the state of a single optimizer to the device.\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/optimizer.py?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m p, v \u001b[39min\u001b[39;00m optimizer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/optimizer.py?line=32'>33</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstate[p] \u001b[39m=\u001b[39m apply_to_collection(v, torch\u001b[39m.\u001b[39;49mTensor, move_data_to_device, device)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:107\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=104'>105</a>\u001b[0m out \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=105'>106</a>\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=106'>107</a>\u001b[0m     v \u001b[39m=\u001b[39m apply_to_collection(\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=107'>108</a>\u001b[0m         v, dtype, function, \u001b[39m*\u001b[39;49margs, wrong_dtype\u001b[39m=\u001b[39;49mwrong_dtype, include_none\u001b[39m=\u001b[39;49minclude_none, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=108'>109</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=109'>110</a>\u001b[0m     \u001b[39mif\u001b[39;00m include_none \u001b[39mor\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=110'>111</a>\u001b[0m         out\u001b[39m.\u001b[39mappend((k, v))\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:99\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=96'>97</a>\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=97'>98</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=98'>99</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=100'>101</a>\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=102'>103</a>\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:354\u001b[0m, in \u001b[0;36mmove_data_to_device\u001b[0;34m(batch, device)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=350'>351</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=352'>353</a>\u001b[0m dtype \u001b[39m=\u001b[39m (TransferableDataType, Batch) \u001b[39mif\u001b[39;00m _TORCHTEXT_LEGACY \u001b[39melse\u001b[39;00m TransferableDataType\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=353'>354</a>\u001b[0m \u001b[39mreturn\u001b[39;00m apply_to_collection(batch, dtype\u001b[39m=\u001b[39;49mdtype, function\u001b[39m=\u001b[39;49mbatch_to)\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:99\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=96'>97</a>\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=97'>98</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=98'>99</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=100'>101</a>\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=102'>103</a>\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m~/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:347\u001b[0m, in \u001b[0;36mmove_data_to_device.<locals>.batch_to\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=344'>345</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, torch\u001b[39m.\u001b[39mTensor) \u001b[39mand\u001b[39;00m device \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _CPU_DEVICES:\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=345'>346</a>\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mnon_blocking\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=346'>347</a>\u001b[0m data_output \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mto(device, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=347'>348</a>\u001b[0m \u001b[39mif\u001b[39;00m data_output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/tudor/.mambaf/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py?line=348'>349</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m data_output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "MODEL='microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
    "data = DataModule(data_path=r'../data/drugs_train.csv', model_name=MODEL)\n",
    "\n",
    "model = BaseBert(model=MODEL, n_classes=10)\n",
    "trainer = Trainer(\n",
    "        gpus=1,\n",
    "        max_epochs=1,\n",
    "    )\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc7067ad0587ee2533d0568fff1e23349b43564da49d2e99835e9871e4935780"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
